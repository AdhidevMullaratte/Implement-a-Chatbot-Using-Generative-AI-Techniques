{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: numpy in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (2.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio numpy nltk matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepi\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load Cornell Dataset\n",
    "def load_cornell_data(path='cornell_movie_dialogs_corpus/movie_lines.txt'):\n",
    "    lines = open(path, encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    conversations = []\n",
    "    for line in lines:\n",
    "        parts = line.split(\" +++$+++ \")\n",
    "        if len(parts) == 5:\n",
    "            conversations.append(parts[-1])\n",
    "    return conversations\n",
    "\n",
    "# Use only pairs\n",
    "def create_pairs(lines):\n",
    "    pairs = []\n",
    "    for i in range(len(lines)-1):\n",
    "        pairs.append((lines[i], lines[i+1]))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,]+\", \" \", text)\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_vocab(pairs):\n",
    "    word2idx = {'<PAD>':0, '<SOS>':1, '<EOS>':2, '<UNK>':3}\n",
    "    idx2word = {0:'<PAD>', 1:'<SOS>', 2:'<EOS>', 3:'<UNK>'}\n",
    "    idx = 4\n",
    "    for pair in pairs:\n",
    "        for sentence in pair:\n",
    "            for word in simple_tokenizer(sentence):\n",
    "                if word not in word2idx:\n",
    "                    word2idx[word] = idx\n",
    "                    idx2word[idx] = word\n",
    "                    idx += 1\n",
    "    return word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(sentence, word2idx):\n",
    "    tokens = simple_tokenizer(sentence)\n",
    "    indices = [word2idx.get(w, word2idx['<UNK>']) for w in tokens]\n",
    "    indices = [word2idx['<SOS>']] + indices + [word2idx['<EOS>']]\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, emb_size)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(pair, encoder, decoder, enc_opt, dec_opt, loss_fn, word2idx):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    src_tensor = sentence_to_tensor(pair[0], word2idx)\n",
    "    tgt_tensor = sentence_to_tensor(pair[1], word2idx)\n",
    "\n",
    "    enc_opt.zero_grad()\n",
    "    dec_opt.zero_grad()\n",
    "\n",
    "    hidden = encoder(src_tensor)\n",
    "    input = torch.tensor([[word2idx['<SOS>']]])\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(1, tgt_tensor.size(1)):\n",
    "        output, hidden = decoder(input, hidden)\n",
    "        loss += loss_fn(output, tgt_tensor[0][t].unsqueeze(0))\n",
    "        input = tgt_tensor[:, t].unsqueeze(1)\n",
    "\n",
    "    loss.backward()\n",
    "    enc_opt.step()\n",
    "    dec_opt.step()\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, word2idx, idx2word, max_len=15):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_tensor = sentence_to_tensor(sentence, word2idx)\n",
    "        hidden = encoder(src_tensor)\n",
    "        \n",
    "        input = torch.tensor([[word2idx['<SOS>']]])\n",
    "        decoded_words = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = decoder(input, hidden)\n",
    "            top1 = output.argmax(1)\n",
    "            word = idx2word.get(top1.item(), '<UNK>')\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "            decoded_words.append(word)\n",
    "            input = top1.unsqueeze(1)\n",
    "        \n",
    "        return ' '.join(decoded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 65.9803\n",
      "Epoch 2 Loss: 42.1352\n",
      "Epoch 3 Loss: 25.6744\n",
      "Epoch 4 Loss: 11.0631\n",
      "Epoch 5 Loss: 3.9344\n"
     ]
    }
   ],
   "source": [
    "lines = load_cornell_data()\n",
    "pairs = create_pairs(lines[:1000])  # Keep it small for demo\n",
    "word2idx, idx2word = build_vocab(pairs)\n",
    "\n",
    "input_size = output_size = len(word2idx)\n",
    "emb_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "encoder = Encoder(input_size, emb_size, hidden_size)\n",
    "decoder = Decoder(output_size, emb_size, hidden_size)\n",
    "\n",
    "enc_opt = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_opt = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train on small set\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for pair in pairs[:100]:\n",
    "        loss = train_step(pair, encoder, decoder, enc_opt, dec_opt, loss_fn, word2idx)\n",
    "        total_loss += loss\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hello!\n",
      "Bot: did you change your hair?\n"
     ]
    }
   ],
   "source": [
    "def chat_step(user_input):\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        return \"Chat ended.\"\n",
    "    return evaluate(encoder, decoder, user_input, word2idx, idx2word)\n",
    "\n",
    "# Example interaction:\n",
    "print(\"You: Hello!\")\n",
    "print(\"Bot:\", chat_step(\"Hello!\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
